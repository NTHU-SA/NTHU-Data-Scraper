name: Run Scraper and Deploy

on:
  schedule:
    - cron: '0 16 * * *'  # 每天 UTC+8 的 00:00 執行（UTC 16:00 對應 UTC+8 00:00）
  push:
    branches:
      - main
  workflow_dispatch:  # 手動觸發

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
            python-version: '3.12'

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Scrape Bus Data
        run: python scrapers/buses.py || true
    
      - name: Scrape Dining Data
        run: python scrapers/dining.py || true

      #- name: Scrape Directory Data
      #  run: python scrapers/directory.py || true

      #- name: Scrape Maps Data
      #  run: python scrapers/maps.py || true

      #- name: Scrape Newsletter Data
      #  run: python scrapers/newsletter.py || true

      - name: Combine JSON Files
        run: python combine.py || true

      - name: Prepare Docs Folder
        run: |
            mkdir -p docs
            rm -rf docs/*
            cp -r json/. docs/
            python folder.py || true
            cp -r docs /tmp/docs

      - name: Deploy to gh-pages Branch
        run: |
            git config --global user.name "github-actions"
            git config --global user.email "github-actions@github.com"

            # 切換到 pages 分支，若不存在則建立
            git fetch origin pages || true
            git checkout pages 2>/dev/null || git checkout --orphan pages
            
            git add .
            git commit -m "action: Update scraped data" || echo "No changes to commit"
            git push --force origin gh-pages
    
